
# ğŸ™ï¸ Text-to-Audio: FastSpeech2 + HiFi-GAN Based TTS System

Welcome to the **Text-to-Audio-TTS** repository â€” a custom-built, modular, and research-inspired project that transforms raw English text into natural-sounding speech using a combination of **FastSpeech2** for spectrogram generation and **HiFi-GAN** as a vocoder for waveform synthesis.

> ğŸ”§ Entire pipeline handcrafted in Python, with no third-party UI tools or automated dashboards. This repository reflects a complete hands-on implementation.

---

## ğŸ“ Repository Overview

```bash
Text-to-Audio-TTS/
â”‚
â”œâ”€â”€ inference.py                  # Real-time TTS inference pipeline
â”œâ”€â”€ train.py                      # HiFi-GAN training script
â”œâ”€â”€ dataset.py                    # Dataset loader and preprocessor
â”œâ”€â”€ Text_to_Audio_Notebook.ipynb # Jupyter notebook for end-to-end experimentation
â”‚
â”œâ”€â”€ model/                        # FastSpeech2 & HiFi-GAN model architectures
â”œâ”€â”€ checkpoints/                 # Pretrained model weights (local only)
â”œâ”€â”€ data_hifigan_validated/      # Sample wav/mel data used for inference
â”œâ”€â”€ outputs/                      # Generated spectrograms and .wav outputs
â”‚
â”œâ”€â”€ training_loss.png             # Training visualization
â”œâ”€â”€ Figure_1.png                  # Sample spectrogram output
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This documentation file
```

---

## ğŸ” Objective

To build a robust, real-time Text-to-Speech (TTS) system that combines:

- ğŸ§  **FastSpeech2** â€” for fast, robust, and accurate mel-spectrogram generation.
- ğŸ”Š **HiFi-GAN** â€” for high-fidelity audio waveform synthesis from spectrograms.

---

## ğŸ§ª Technologies Used

- `Python 3.x`
- `PyTorch` for deep learning model training
- `NumPy`, `Librosa` for audio processing
- `Matplotlib` and `Seaborn` for training and output visualizations

---

## ğŸ§¾ Dataset Used

This project utilizes a subset of the open-source [LJSpeech-1.1 dataset](https://keithito.com/LJ-Speech-Dataset/) (â­¢ **Kaggle mirror available** [here](https://www.kaggle.com/datasets/mervemenekse/ecommerce-dataset)). Due to GitHub's file size restrictions, only a small set of **mel-spectrograms** and **wav files** are uploaded in the `data_hifigan_validated/` folder.

To train or evaluate the full system:

1. Download the full dataset (LJSpeech-1.1).
2. Preprocess into aligned `wavs/` and `mel/` pairs.
3. Place them inside the appropriate `/data` folder.

---

## âš™ï¸ Setup & Installation

```bash
git clone https://github.com/NPK05/Text-to-Audio-TTS.git
cd Text-to-Audio-TTS
pip install -r requirements.txt
```

---

## ğŸš€ Run Inference

```python
from inference import text_to_speech

text_to_speech("This is a sample text-to-speech output using FastSpeech2 and HiFi-GAN.")
```

---

## ğŸ“Š Output Samples

| ğŸ§  Feature                  | ğŸ“ˆ Output |
|---------------------------|-----------|
| Training Curve            | ![training_loss](training_loss.png) |
| Mel Spectrogram Sample    | ![mel](Figure_1.png) |
| Final Audio Waveform      | Stored in `/outputs` as `.wav` files |

---

## ğŸ” Checkpoints & Models

Due to the 25MB GitHub file limit:
- ğŸ”’ Full pretrained model weights are **excluded** from the repo.
- You can download trained checkpoints or generate your own using `train.py`.

Place any trained `.pt` or `.ckpt` models under:

```
Text-to-Audio-TTS/
â””â”€â”€ checkpoints/
    â””â”€â”€ generator.pt
    â””â”€â”€ discriminator.pt
```

---

## ğŸ§  Core Features

- ğŸ”¤ **Text Preprocessing** â€” Converts raw text to phoneme-based input
- ğŸ“ˆ **Mel Spectrogram Generator** â€” FastSpeech2 decoder
- ğŸ”Š **Waveform Generation** â€” HiFi-GAN generator architecture
- ğŸ¯ **Modular Inference** â€” Easily test and modify
- ğŸ› ï¸ **Custom Training Pipeline** â€” Manual control over epochs, loss, and batch size

---

## ğŸ“Œ Design Philosophy

This project was built as a complete learning implementation with no drag-and-drop UI tools or auto-generated dashboards. Every component from model loading to spectrogram visualization was manually crafted in Python.

ğŸ“š **Inspired by**:
- _Neural Speech Synthesis with Transformer Network_ â€“ Microsoft
- _HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis_
- Kaggle shared notebooks and open GitHub repositories

---

## ğŸ§  Suggested Enhancements

- Integrate Tacotron2 as an optional frontend.
- Use Griffin-Lim as an alternative vocoder for fast experiments.
- Improve phoneme-to-spectrogram alignment and duration modeling.

---

## ğŸ“˜ References & Learning Sources

- ğŸ”— [HiFi-GAN GitHub](https://github.com/jik876/hifi-gan)
- ğŸ“˜ *Deep Learning for Speech and Language* â€“ Li Deng
- ğŸ“• *Python Machine Learning* â€“ Sebastian Raschka
- ğŸ§ª Blog posts on [Medium](https://medium.com/), [Kaggle](https://www.kaggle.com/), and official [PyTorch tutorials](https://pytorch.org/tutorials/)

---

## ğŸ™Œ Acknowledgments

Special thanks to:
- The **Kaggle** community for shared insights and baseline models.
- **GPT** for initial code restructuring and formatting assistance.
- All research authors and contributors to TTS systems.

---

## ğŸ“„ License

This project is available under the **MIT License** for educational and non-commercial use.

---

### ğŸš€ Enjoy building next-gen audio AI from scratch!
